{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Configure notebook to use external packages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Spark community contributes a lot of packages that extend Spark. These packages might not be available out of the box in the Spark distribution that you are using. Here is an example of how to use **spark-csv**, a CSV data source for Spark, in a notebook using the `%%configure` magic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%configure\n",
    "{ \"conf\": {\"spark.jars.packages\": \"com.databricks:spark-csv_2.11:1.5.0\" }}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the snippet above, `packages` expects a list of maven coordinates in [Maven Central Repository](http://search.maven.org/). In this snippet, `com.databricks:spark-csv_2.11:1.5.0` is the maven coordinate for **spark-csv** package.\n",
    "\n",
    "You can search for all the available packages in the Maven repository. You can also get a list of the available packages from other sources. For example, a complete list of community-contributed packages is available at [Spark Packages](http://spark-packages.org).\n",
    "\n",
    ">Note:\n",
    ">- HDInsight Spark cluster runs Scala 2.11. If you are using a library written in Scala, make sure you use the version of the package that is built for Scala 2.11.\n",
    ">- In notebooks that use external packages, make sure you call the `%%configure` magic in the first code cell. This ensures that the kernel is configured to use the package before the session starts. If you forget to configure the kernel as the first step, you can always issue the `%%configure` with the `-f` parameter, but that will restart the session and all progress will be lost. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's a simple snippet that demonstrates how to use the spark-csv package for constructing a DataFrame from a csv file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "val df = spark.read.format(\"com.databricks.spark.csv\").\n",
    "        option(\"header\", \"true\").\n",
    "        option(\"inferSchema\", \"true\").\n",
    "        load(\"wasb:///HdiSamples/HdiSamples/SensorSampleData/hvac/HVAC.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df.select(\"Time\").count()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Spark",
   "language": "",
   "name": "sparkkernel"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
